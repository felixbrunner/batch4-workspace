{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79f2337e7779945a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU7 - Regression With Linear Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62ddf765d4352694",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following:\n",
    "     - Simple Linear Regression\n",
    "     - Multiple Linear Regression\n",
    "     - Closed Form Solution\n",
    "     - Using scikit learn linear regression implementations\n",
    "     - Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cb0eb9c3a32286f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Base imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12e70b6fe3afbe19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's imagine you just landed your first Data Science job in your dream company and you are super excited and wants everything to be perfect and you want to give 200% everyday and ...\n",
    "\n",
    "\n",
    "![its-happening](assets/its-happening.gif)\n",
    "\n",
    "\n",
    "Your are so excited for your first day.... and then you see your boss! He's walking in your direction... \n",
    "\n",
    "You start to feel the bad buterflies in your stomach and he now stands in front of you:\n",
    "\n",
    "\n",
    "![got_a_job](assets/got_a_job2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74ff3f984bb74106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1eba3d354cdb022",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "He wants to hire a manager with 5 years of experience so as a first task, he asked you:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ab1771b81169a1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<center><em>\"What is the best salary I should offer to him?\" </em></center>\n",
    "\n",
    "He provides the company's data about the current salary distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8fc43cd0410a8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_salary = pd.read_csv('data/salary_data.csv')\n",
    "df_salary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32f343233539ec16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5687e6f07d6725ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_salary = df_salary.sort_values('YearsExperience')\n",
    "plt.xlim((0, 12))\n",
    "plt.ylim((20000, 130000))\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.title('Salary based on Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.plot(df_salary['YearsExperience'], df_salary['Salary'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1493f884ebfd688a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To perform this task, you will need to use linear regression to solve this simple problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67395307d5b7356e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.1 - Simple Linear Model\n",
    "\n",
    "As you can see, our data has only one variable ($x$: 'Years of Experience') and one label ($y$: 'Salary'), so we can try to fit it with a simple linear regression. This model is represented by the following expression:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $\\hat{y}$ are the predictions, $\\beta_0$ is the intercept, $\\beta_1$ is the coefficient and $x$ is the input sample. Expanding to several samples, we can write this equation in a vector form:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\beta_1 \\vec{x}$$\n",
    "\n",
    "Implement the function <em>simple_linear_regression_model</em>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0658efc6a6964c6e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression_model(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples,) - The input data \n",
    "        betas: numpy.array with shape (2,) - The weights of the model [b_0, b_1]\n",
    "    \n",
    "    Returns:\n",
    "        f1, f2 : numpy.array with shape (num_samples,) - intermediate calculations\n",
    "        y_hat : list (lenght = (num_samples)) - The prediction made by \n",
    "                the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's begin with the first term of the equation\n",
    "    # f1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, the second term\n",
    "    # f2 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now let's put all together\n",
    "    # y_hat = ...\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return f1, f2, y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-520e93f68c350db0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check if your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-651ae1ad52511300",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "f1_1, f2_1 ,y_hat1 = simple_linear_regression_model(np.arange(0, 5), np.array([-15, 20]))\n",
    "expected_hash_1 = 'ef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d'\n",
    "expected_hash_2 = 'b63d2b235e273730eda06df31b5d8c0f4c73eec62deb0a3937bc3540384d6a26'\n",
    "expected_hash_3 = '981a391f15355b5d3f9fa774f7e5e2080b001ddf5e358b2b18c1066ce44dbd9e'\n",
    "assert hashlib.sha256(str(len(f1_1)).encode('utf-8')).hexdigest() == expected_hash_1, \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(f1_1[-1]).encode('utf-8')).hexdigest() == expected_hash_2, \"Make sure to use the beta_0 and the right dimension of x \"\n",
    "assert hashlib.sha256(str(f1_1[0]).encode('utf-8')).hexdigest() == expected_hash_2, \"Make sure to use the beta_0 and the right dimension of x\"\n",
    "assert hashlib.sha256(str(len(f2_1)).encode('utf-8')).hexdigest() == expected_hash_1, \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(type(y_hat1)).encode('utf-8')).hexdigest() == expected_hash_3, \"Pay attention to the type of the output\"\n",
    "np.testing.assert_array_almost_equal(y_hat1, np.array([-15.0, 5.0, 25.0, 45.0, 65.0]))\n",
    "\n",
    "# Test using our dataset\n",
    "f1_2, f2_2 ,y_hat2 = simple_linear_regression_model(df_salary['YearsExperience'],np.array([3.0, 2.5]))\n",
    "expected_hash_4 = '624b60c58c9d8bfb6ff1886c2fd605d2adeb6ea4da576068201b6c6958ce93f4'\n",
    "expected_hash_5 = 'a416ea84421fa7e1351582da48235bac88380a337ec5cb5a9239dc7d57908b4b'\n",
    "expected_hash_6 = '981a391f15355b5d3f9fa774f7e5e2080b001ddf5e358b2b18c1066ce44dbd9e'\n",
    "assert hashlib.sha256(str(len(f1_2)).encode('utf-8')).hexdigest() == expected_hash_4, \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(f1_2[-1]).encode('utf-8')).hexdigest() == expected_hash_5, \"Make sure to use the beta_0 and the right dimension of x \"\n",
    "assert hashlib.sha256(str(f1_2[0]).encode('utf-8')).hexdigest() == expected_hash_5, \"Make sure to use the beta_0 and the right dimension of x\"\n",
    "assert hashlib.sha256(str(len(f2_2)).encode('utf-8')).hexdigest() == expected_hash_4, \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(str(type(y_hat2)).encode('utf-8')).hexdigest() == expected_hash_6, \"Pay attention to the type of the output\"\n",
    "np.testing.assert_array_almost_equal(np.array(y_hat2)[[2,8,11,15,20, 23]].tolist(), \n",
    "   [6.75, 11.0, 13.0, 15.25, 20.0, 23.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2f4bcb7e195509c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ok, so know that you have a function to construct the model, the next step is to discover the values of betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc935eebba6de6f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first approach you can take is to implement the closed form solution. This is, solving the equation that minimizes the error accross all of the samples - ordinary least squares. For that, however, we need to understand what the error trying to be minimized is. Let's take a look at the error function you learned:\n",
    "\n",
    "### Exercise 1.2 Summed Square Error\n",
    "\n",
    "Start by implementing the error function presented - summed squared error:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N e_i^2 = \\frac{1}{N} \\sum_{n=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where the error is the difference between your predictions and the actual sample value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1049431fa099d4b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def summed_squared_error(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy.array with shape (num_samples, ) - real target\n",
    "        y_hat : numpy.array  with shape (num_samples, ) - predicted target\n",
    "    \n",
    "    Returns:\n",
    "        mean_squared_error : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the error\n",
    "    # error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, square the difference\n",
    "    # squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Finally, take the mean and return the error\n",
    "    # mean_squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return error, squared_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f2a4da7fe193783",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4510d1dc5df769f3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "np.testing.assert_almost_equal(summed_squared_error(np.array([.3445]), np.array([.232]))[2], 0.01265624999999999)\n",
    "np.testing.assert_almost_equal(summed_squared_error(np.array([.3445]), np.array([.232]))[0], 0.1125)\n",
    "np.testing.assert_almost_equal(summed_squared_error(np.array([2.1431]), np.array([4.5313]))[2], 5.703499239999999)\n",
    "np.testing.assert_almost_equal(summed_squared_error(np.array([2.1431]), np.array([4.5313]))[1], 5.70349924)\n",
    "\n",
    "# Test using our dataset\n",
    "x_rnd = df_salary['YearsExperience'].values\n",
    "y_rnd = df_salary['Salary'].values\n",
    "beta_rnd = np.array([120000, -12000])\n",
    "_, _,y_hat_rnd = simple_linear_regression_model(x_rnd, beta_rnd)\n",
    "\n",
    "np.testing.assert_almost_equal(summed_squared_error(y_rnd, y_hat_rnd)[0][3], -52475.0)\n",
    "np.testing.assert_almost_equal(summed_squared_error(y_rnd, y_hat_rnd)[1][8], 294294025.0)\n",
    "np.testing.assert_almost_equal(summed_squared_error(y_rnd, y_hat_rnd)[2], 4003804070.733333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab4aadf231356588",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see from the previous test, picking just random values for our weights will probably yield very high error values. You can even visualize this to see that in fact these random weights don't fit our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3963d719eccbc48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((0, 12))\n",
    "plt.ylim((20000, 130000))\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.ylabel('Salary')\n",
    "plt.title('Prediction with random weights')\n",
    "plt.plot(df_salary['YearsExperience'], df_salary['Salary'], 'b.', label='True')\n",
    "plt.plot(df_salary['YearsExperience'], y_hat_rnd, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b7d6627f93ad176",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.3 - Closed Form Solution\n",
    "\n",
    "Let's then implement a closed form solution. Remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i}^{N}{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i}^{N}{(x_i - \\bar{x})^2}} = \\frac{cov(x, y)}{var(x)}$$\n",
    "\n",
    "with cov(x,y) and var(x) are, respectively, the covariance and variance of the samples\n",
    "\n",
    "$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{N}\\sum_{i}^{N}{y_i}$ and $\\bar{x} = \\frac{1}{N}\\sum_{i}^{N}{x_i}$ are the means of the sample.\n",
    "\n",
    "Complete the closed form solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c6179999833a9a8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_closed_form_solution_function(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, ) - input samples \n",
    "        y : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        beta_0: float\n",
    "        beta_1: float\n",
    "    \"\"\"\n",
    "    # The sample covariance and variance for 1-d arrays in \n",
    "    # numpy for this particular case are computed as follows\n",
    "    # We covered this part so you don't lose too much time on these details\n",
    "    cov_xy = np.cov(x, y, bias=True)[0][1]\n",
    "    var_x = np.var(x)\n",
    "    \n",
    "    # Compute coefficient beta_1\n",
    "    # beta_1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute intersect beta_0\n",
    "    # beta_0 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, return the coefficients (betas)\n",
    "    \n",
    "    return beta_0, beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea54ae86dbd717c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed0cf5f5ace73f0a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    simple_closed_form_solution_function(np.arange(0, 10), np.arange(0, 20, 2)),\n",
    "    (0.0, 2.0)\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    simple_closed_form_solution_function(np.arange(-2, 3), np.array([-1.5, -.56, .26, 1.3, 2.5])),\n",
    "    np.array([.4, .986])\n",
    ")\n",
    "\n",
    "#Test using our dataset\n",
    "x_cf = df_salary['YearsExperience'].values\n",
    "y_cf = df_salary['Salary'].values\n",
    "beta_cf = simple_closed_form_solution_function(x_cf, y_cf)\n",
    "y_hat_cf = simple_linear_regression_model(x_cf, beta_cf)[2]\n",
    "\n",
    "np.testing.assert_array_almost_equal(summed_squared_error(y_cf, y_hat_cf)[2], 31270951.722280964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87eed9f8fb8bc294",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Okay... Do you remember the question that your boss asked you, related to a worker with 5 years of experience: <em>\"What is the best salary I should offer to him?\"</em>\n",
    "\n",
    "We now are capable of answer to that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63a7d5b792053937",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can use the same code of above to create the variables we need\n",
    "x_simple = df_salary['YearsExperience'].values\n",
    "y_simple = df_salary['Salary'].values\n",
    "\n",
    "#Let's calculate the best coefficients\n",
    "beta_simple = simple_closed_form_solution_function(x_simple, y_simple)\n",
    "\n",
    "# Now, let's calculate the predicted salary using one of the above functions we already implemented above\n",
    "# Hint: If you don't remember the types/shapes of input/output of the function, it will be a good idea to check.\n",
    "# y_hat_simple = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e90b401208965c6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(y_hat_simple, list), \"Remember that the function returns 3 outputs\"\n",
    "np.testing.assert_array_almost_equal(summed_squared_error(y_simple, y_hat_simple)[2], 735266712.818672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69929e1aef63748a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can also visualize how good your solution fits the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61a07e39f9f8fdd4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((0, 12))\n",
    "plt.ylim((20000, 130000))\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.ylabel('Salary')\n",
    "plt.title('Predicion with best pred with closed form solution')\n",
    "plt.plot(df_salary['YearsExperience'], df_salary['Salary'], 'b.', label='True')\n",
    "plt.plot(df_salary['YearsExperience'], y_hat_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28ca0978a921d6b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You sent an email to your boss with the answer and you just deliver your first taks!\n",
    "\n",
    "![mission](assets/mission.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb49a1b1ed811039",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6c1ed785a80aad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While you were searching to complete the previous task, you read something about multiple linear regression using polynomial functions. \n",
    "You have a few hours of free time and you decided to implement a simple model of a multiple linear regression so you could be better prepared in case your boss wants you to do a task where you need this knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-791ecdf680235b04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-708eb8c290452c85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this section, we will expand what we learned to a linear regression with multiple inputs - which we call features of our model. We will use a very specific scenario so we are able to visualize it better - we will try to model a polynomial function, in particular, a cubic function, which can be written as:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^{2} + \\beta_3 x^{3}$$\n",
    "\n",
    "You will basically be considering each power of x as a different feature. To simplify, we are going to construct a dataset with the powers we want for this, so let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-349d6ec88f5f307d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transforming the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "polynomial_features= PolynomialFeatures(degree=3)\n",
    "x_poly = polynomial_features.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cee8fdcad5e51a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml = pd.DataFrame(x_poly, columns=['x1', 'x2', 'x3', 'x4']).drop(['x1'], axis=1)\n",
    "data_ml.columns = ['x0', 'x1', 'x2']\n",
    "data_ml['y'] = pd.DataFrame(y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efe0377dc84a5c29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e2ef3862e8aae7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml = data_ml.sort_values('x0')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Relation between x0 and y')\n",
    "plt.plot(data_ml['x0'], data_ml['y'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9f9d3c78f620b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Although this model is non linear in its features, notice that it is linear with respect to the weigths, and the equation above can be rewritten as\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_0 + \\beta_2 x_1 + \\beta_3 x_2$$\n",
    "\n",
    "where $[x_0, x_1, x_2]$ is our feature vector for a given sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79fedc69b50dbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.1  Linear Model Extended\n",
    "\n",
    "The multiple linear regression problem is just the linear regression problem on a linear model with several inputs. This model can be represented by the following expressions:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^K \\beta_i x_i$$\n",
    "\n",
    "We can also write it in matrix form to consider several samples, as before:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\vec{\\beta_{1-k}}X^T$$\n",
    "\n",
    "where X is now a matrix, containing all features for all samples: \n",
    "\n",
    "$$ X = \\begin{bmatrix} \n",
    "x_1^1 & x_1^2 & ... & x_1^k \\\\\n",
    "x_2^1 & x_2^2 & ... & x_2^k \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_n^1 & x_n^2 & ... & x_n^k \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "If you go back to the closed form solution you implemented before, you might notice that we had already used matrix form, in particular to concatenate our whole weight vector. We'll follow the same logic, and extend our matrix X to allow a collumn of ones:\n",
    "\n",
    "$$ X' = [\\vec{1} | X] $$\n",
    "\n",
    "and rewrite:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\vec{\\beta}(X')^T$$\n",
    "\n",
    "Implement below this extended model.\n",
    "\n",
    "Tip: You might want to review the learning notebook and examples to get used to the matrix handling in the following problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10eb3e761476d6d9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_linear_model_function(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        betas : numpy.array with shape (num_features + 1,) - weights of \n",
    "                our model, with the intercept in the first position of \n",
    "                the array\n",
    "    \n",
    "    Returns:\n",
    "        y_pred : list(len= (num_samples)) - prediction \n",
    "                made by the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We do the proper reshaping of weights so you don't have \n",
    "    # to worry about that and focus on the remaining logic\n",
    "    betas = betas.reshape((1, -1))\n",
    "    \n",
    "    # Extend the matrix x with a collumn of ones\n",
    "    # X_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Compute the output of the linear model\n",
    "    # y_pred = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return y_pred.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a1c4e3d2ce2183d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Since this is an extension of the simple linear model, it should be able to cover that use case also. Check that your solution still passes the test for the simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d853a799cc5992d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(np.arange(0, 10).reshape(-1, 1), np.array([-12, 30])), \n",
    "    [-12.,  18.,  48.,  78., 108., 138., 168., 198., 228., 258.])\n",
    "\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(np.arange(-5, 5).reshape(-1, 1), np.array([1, 1])), \n",
    "    [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(np.arange(-10, 10, 2).reshape(-1, 1), np.array([0.25, 2.1])), \n",
    "    [-20.75, -16.55, -12.35, -8.15, -3.95, 0.25, 4.45, 8.65, 12.85, 17.05]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a8e58f5e96ecf1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now check your solution passes the tests for the extended version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c9cab5fa1ebfc99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(np.array([[1., 2.], [3., 4.], [5., 6.]]), np.array([-1., 0., 1.])), \n",
    "    [1., 3., 5.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(np.ones((10, 2)), np.array([1., 2., 3.])), \n",
    "    [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae733317d24d320d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.2 - Summed Squared Error\n",
    "\n",
    "As before, we can use the summed squared error as our cost. However, since this function does not receive anything other than the predictions and true values, there is no need to reimplement it. As before, let's see how the model would behave and what would be its error if we pick random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebb1e0761f537021",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Error in our dataset with random weights\n",
    "x_pln_rnd = data_ml['x0']\n",
    "X_pln_rnd = data_ml.drop('y', axis=1).to_numpy()\n",
    "y_pln_rnd = data_ml['y'].values\n",
    "beta_pln_rnd = np.array([1., 1., -.5, 0.])\n",
    "y_hat_pln_rnd = extended_linear_model_function(X_pln_rnd, beta_pln_rnd)\n",
    "\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.plot(x_pln_rnd, y_pln_rnd, 'b.', label='true')\n",
    "plt.plot(x_pln_rnd, y_hat_pln_rnd, 'r-', label='pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Error: {}'.format(summed_squared_error(y_pln_rnd, y_hat_pln_rnd)[2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e3366d377d3c9b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the solution is clearly not a fit, and the error is very high. So let's move into our closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ff21e3c52fafce2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.3 - Closed Form Solution\n",
    "\n",
    "Let's now implement the closed form solution for the generic case. When put into matrix form, remember the solution to minimize the error can be written as:\n",
    "\n",
    "$$ \\vec{\\beta} = (X^TX)^{-1}(X^T\\vec{y})$$\n",
    "\n",
    "\n",
    "Where X is our matrix of samples extended to add a 1 component in each sample, $X = [\\vec{1} | X] $ , $\\vec{y}$ is the output vector, and $\\vec{\\beta}$ the weight vector with weights $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Implement the closed form solution for the multiple linear regression problem below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e87f47eb6ec320ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_closed_form_solution_function(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples, ) - sample labels\n",
    "    \n",
    "    Returns:\n",
    "        betas : list (len= (num_features + 1)) - weight vector \n",
    "    \"\"\" \n",
    "    \n",
    "    # Proper reshaping of the labels\n",
    "    y = y.reshape((-1, 1))\n",
    "\n",
    "    # Extend vector of samples with array of ones\n",
    "    # X_extended = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute betas\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return betas.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ec27afe2a6ee23b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-47587567e05c2602",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Old tests\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution_function(np.arange(0, 10).reshape(10, 1), np.arange(0, 20, 2).reshape(10, 1)),\n",
    "    [0., 2.]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution_function(np.arange(-2, 3).reshape(5, 1), np.array([-1.25, -.5, .25, 1., 1.75]).reshape(5, 1)),\n",
    "    [.25, .75]\n",
    ")\n",
    "\n",
    "\n",
    "# Extended test cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution_function(np.array([[1., -1.], [2., 1.], [3., -5.]]), np.array([0., 1., 0.])), \n",
    "    [-0.25, 0.5, 0.25]\n",
    ")\n",
    "\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution_function(np.array([[10., -2.], [-4., 5.], [-7., -8.]]), np.array([2., 1., -.5])), \n",
    "    np.array([1.019704, 0.115764, 0.08867])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbdc5cd776150269",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now apply it to our dataset to get the best weights, and measure the error across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd38814320080448",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_pln_cf = data_ml['x0'].values\n",
    "y_pln_cf = data_ml['y'].values\n",
    "\n",
    "# All collumns except y\n",
    "X_pln_cf = data_ml.drop('y', axis=1).to_numpy()\n",
    "\n",
    "beta_pln_cf = extended_closed_form_solution_function(X_pln_cf, y_pln_cf)\n",
    "y_hat_pln_cf = extended_linear_model_function(X_pln_cf, np.array(beta_pln_cf))\n",
    "\n",
    "assert math.isclose(summed_squared_error(y_pln_cf, y_hat_pln_cf)[2], 7.159451499759534), \"Check your closed form function!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d204ff04dacb5dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "And finally we'll try to see how well this solution fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33d9aab23eb2dadd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicion with best pred with closed form solution for multiple linear regression')\n",
    "plt.plot(x_pln_cf, y_pln_cf, 'b.', label='True')\n",
    "plt.plot(x_pln_cf, y_hat_pln_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ecb84f49c6e98b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Much better! You have a much better fit.\n",
    "\n",
    "![reaction](assets/reaction.gif)\n",
    "\n",
    "You are just feeling great! \n",
    "\n",
    "Your day is finally over and you have an email from your boss with a new task: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3be83be929faed69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<center> <em> \"I have another task for you! I want to find out how to distribute my budget across several departments so I can get the most profit! This is very important, new one , so don't screw this up! </em> </center>\n",
    "\n",
    "\n",
    "You open the dataset that he sent to you and you realize that you need a more robust implementation than the closed form solution and error functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bbf62d226129f9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3 . ScikitLearn Linear Regression\n",
    "\n",
    "Luckily, ScikitLearn already provides us with a solver for the Linear Regression problem, which implements a closed form solution internally. It also provides already some extra info on the regression, such as the $R^2$ score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cc093acaeccc281",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/50_Startups.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cc95feb70c42b3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# There is a column \"State\" that we won't use it as part of this exercise (you will learn how to use it in the \n",
    "# next learning units)\n",
    "\n",
    "# All features except profit and state\n",
    "columns_budget = data.drop(['Profit', 'State'], axis=1)\n",
    "x_budget = columns_budget\n",
    "\n",
    "# Profit\n",
    "y_budget = data['Profit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbd3a2e4e2107803",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Each of the columns in the table is one of the features our model is going to use, this is, one of the inputs we are going to give it. Use the `sklearn.linear_model.LinearRegression` module that you've learned and implement it in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4896df37a4f5670c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_model_regression(x, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        x: numpy.ndarray with shape (num_samples, num_features) - samples of our model\n",
    "        y: numpy.array with shape (num_samples, ) - sample labels\n",
    "        \n",
    "    Return:\n",
    "        coefs: list (len= (num_features) - coefficients vector\n",
    "        intercept: float - intercept value\n",
    "        score: float - R squared score of regression\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the linear regressor\n",
    "    # lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # coefs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the intercept\n",
    "    # intercept = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Calculate the score\n",
    "    # score = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return coefs, intercept, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a710d3ea831ce0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's see then what our coefficients are for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edea816492fd3f98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_budget_skl = x_budget.values.reshape(-1, 3)\n",
    "y_budget_skl = y_budget.values\n",
    "\n",
    "\n",
    "coefs_budget, intercept_budget, score_budget = sklearn_model_regression(x_budget_skl, y_budget_skl)\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.Series(coefs_budget, columns_budget.columns))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept: {}'.format(intercept_budget))\n",
    "print('\\n')\n",
    "\n",
    "print('R² score: {}'.format(score_budget))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aca587618ebd00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfb0a7be54a211de",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "betas_budget = np.concatenate((np.array([intercept_budget]), np.array(coefs_budget)), axis=0)\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(x_budget[:10], betas_budget),\n",
    "    [192521.2528900786, 189156.76823226505, 182147.27909620487, 173696.700025534, 172139.5141832719, \n",
    "     163580.7805712008, 158114.09666864751, 160021.3630478112, 151741.6996986506, 154884.68410994846]\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model_function(x_budget[-10:], betas_budget),\n",
    "    [74815.95399104737, 74802.55623866276, 70620.41182056018, 60167.039963347925, 64611.35491570331, \n",
    "     47650.64968690646, 56166.20685260787, 46490.588983346795, 49171.38815762721, 48215.13411129866]\n",
    ")\n",
    "\n",
    "y_hat_budget = extended_linear_model_function(x_budget, betas_budget)\n",
    "assert math.isclose(summed_squared_error(y_budget, y_hat_budget)[2], 78417126.0191308)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d32d00c1a044a960",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We can also use this model to get to the solution for our previous problems. Run the cells below and see the scikitlearn solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79638baa0ce7e0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df_lin = pd.read_csv('data/salary_data.csv')\n",
    "df_lin = df_lin.sort_values('YearsExperience')\n",
    "x_lin_skl = df_lin['YearsExperience'].values.reshape(-1, 1)\n",
    "y_lin_skl = df_lin['Salary'].values\n",
    "\n",
    "coefs_lin, intercept_lin, _ = sklearn_model_regression(x_lin_skl, y_lin_skl)\n",
    "betas_lin_skl = np.array([intercept_lin, coefs_lin[0]])\n",
    "y_hat_lin_skl = extended_linear_model_function(x_lin_skl, betas_lin_skl)\n",
    "\n",
    "plt.xlim((0, 12))\n",
    "plt.ylim((20000, 130000))\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.ylabel('Salary')\n",
    "plt.title('Prediction with sklearn - Salary/YearsExperience')\n",
    "plt.plot(x_lin_skl, y_lin_skl, 'b.', label='True')\n",
    "plt.plot(x_lin_skl, y_hat_lin_skl, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07a61d128cb2917a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_pln_skl = data_ml['x0'].values.reshape(-1, 1)\n",
    "y_pln_skl = data_ml['y'].values\n",
    "X_pln_skl = data_ml.drop('y', axis=1).to_numpy()\n",
    "\n",
    "coefs_pln, intercept_pln, _ = sklearn_model_regression(X_pln_skl, y_pln_skl)\n",
    "betas_pln_skl = np.concatenate((np.array([intercept_pln]), np.array(coefs_pln)), axis=0)\n",
    "y_hat_pln_skl = extended_linear_model_function(X_pln_skl, betas_pln_skl)\n",
    "\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Prediction with sklearn - MultipleLinearRegression')\n",
    "plt.plot(x_pln_skl, y_pln_skl, 'b.', label='True')\n",
    "plt.plot(x_pln_skl, y_hat_pln_skl, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8b2e2340ecbae20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your co-worker was giving you some hints to help you in this last task. She is very nice and helpful and she suggests that you try a more generic method - the Gradient Descent! You've heard of it so although you are very tired already you give it a chance! \n",
    "\n",
    "Last one before you go home!\n",
    "\n",
    "\n",
    "![tired](assets/tired.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d547f4157ab02c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4. Gradient Descent\n",
    "\n",
    "Now we will see how to get to a similar solution through learning methods. In this section, you will implement gradient descent, an algorithm to iteratively update the weights according to the direction of the error.\n",
    "\n",
    "This method is an iterative process that updates the weights in the direction that minimizes our error. For this it makes use of derivatives. The formula follows:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\Delta_\\vec{\\beta}$ is a vector of the derivatives - also called gradients - of our error function with respect to the weights, $\\beta_{i+1}$ is the updated weight and $\\beta_{i}$ the current weight. We then need to be able to compute these gradients to be able to update the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54fd3a31f51054",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Multiple Linear Regression partial derivatives\n",
    "\n",
    "The vector $\\Delta_\\vec{\\beta}$ in the formula above is just a vector with the partial derivatives of the error function with respect to each of the weights. The formulas for these partial derivatives with respect to each weigth are defined as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_0} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_1} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{1_n} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_K} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{K_n} $$\n",
    "\n",
    "Since the focus of this notebook is for you to implement the methods to solve linear regression, and you already have quite some work, we'll solve this one for you. Check below the code for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-315f5fa32e6031cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_partial_derivatives_function(x, y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,) - sample labels\n",
    "        y_hat : numpy.array with shape (num_samples,) - predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        deltas : pandas.Series shape (num_features + 1,)\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    # Compute the difference between the targets and the predictions.\n",
    "    y_diff = y - y_hat\n",
    "    \n",
    "    # Initialize the numpy array of partial derivatives\n",
    "    deltas = np.zeros((x.shape[1] + 1, ))\n",
    "    \n",
    "    # Compute the partial derivative for b0\n",
    "    deltas[0] = -(2 * y_diff).mean()\n",
    "    \n",
    "    # Extract the partial derivatives of the remaining betas  \n",
    "    for col in range(x.shape[1]): \n",
    "        deltas[col+1] = -((2 * y_diff) * x[:, col]).mean()\n",
    "    \n",
    "    # Return derivatives \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-664e5ef6f8b4ed2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 4.1 Adjusting  parameters with gradient descent\n",
    "\n",
    "Now we want to adjust the weights with the update rule we presented:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\eta$ is our learning rate - how fast we want to move in the direction of the gradient. We will be implementing the standard gradient descent, also know as batch gradient descent, where for each iteration we will compute the derivatives by taking in all the dataset:\n",
    "\n",
    "1. _For epoch in 1...epochs:\n",
    "    1. Predict the outputs with current weights $\\hat{y} = \\vec{\\beta}_i X$\n",
    "    2. $\\Delta_{\\beta_0} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})$\n",
    "    3. $\\Delta_{\\beta_{i=1...N}} = \\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})x_{i_n} $\n",
    "    4. $\\beta_i = \\beta_i - \\eta \\Delta_{\\beta_i}$\n",
    "\n",
    "Notice that you can get the gradients in steps 1 and 2 with the function implemented above. \n",
    "\n",
    "The number of epochs and learning rate will impact how fast and how good the solution we converge to. Besides the number of epochs there are more clever ways of knowing when to stop this procedure, but for simplicity, we will only use this one here.\n",
    "\n",
    "Implement this gradient descent function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f64edb60cc15b1a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent_function(x, y, betas, learning_rate, epochs): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - samples of our model\n",
    "        y : numpy.array with shape (num_samples,)  - sample labels\n",
    "        betas : numpy.array with shape (num_features + 1,) - initial weights\n",
    "        learning_rate : float - factor that will define the size of update step\n",
    "        epochs : int - number of times to run full dataset\n",
    "\n",
    "    Returns:\n",
    "        betas : list (len= (num_features + 1)) - final weights after algorithm\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        # Compute estimates for this iteration \n",
    "        # y_hat = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        y_hat = np.array(y_hat)\n",
    "\n",
    "        # Compute the partial derivatives of the error function \n",
    "        # (hint: check linear_regression_partial_derivatives)\n",
    "        # deltas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Update betas with Gradient Descent rule \n",
    "        # betas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return betas.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9251131ceba84a3f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b26ec5b38f334783",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_budget.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "x_budget_skl = x_budget.values.reshape(-1, 3)\n",
    "y_budget_skl = y_budget.values\n",
    "\n",
    "betas_ = linear_regression_gradient_descent_function(x_budget_skl, y_budget_skl, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    [1.7940083172411833e+96,\n",
    "     1.619051573591378e+101,\n",
    "     2.1984624886437376e+101,\n",
    "     4.766200794544268e+101], \n",
    "    decimal=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84a4265ff22c40c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In order to compare the coefficients between the closed form solution and the result from sklearn model, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8566cbbbdce2c34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Read Dataset\n",
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "\n",
    "columns_housing = data.drop('MEDV', axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "y_housing = data['MEDV'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3aea3af7aafc4f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "\n",
    "betas_ = linear_regression_gradient_descent_function(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "coefs_housing, intercept_housing, score_housing = sklearn_model_regression(x_housing, y_housing)\n",
    "\n",
    "intercept_housing_sgd = betas_[0]\n",
    "coefs_housing_sgd = betas_[1:]\n",
    "\n",
    "series_sgd = pd.Series(coefs_housing_sgd, columns_housing.columns, name='SGD')\n",
    "series_ols = pd.Series(coefs_housing, columns_housing.columns, name='OLS')\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.concat([series_sgd, series_ols], axis=1))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept SGD: {}'.format(intercept_housing_sgd))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept OLS: {}'.format(intercept_housing))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4e45d100fa0d313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is it! The end of your first day and the end of this learning unit! \n",
    "\n",
    "\n",
    "![sum](assets/sum.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
