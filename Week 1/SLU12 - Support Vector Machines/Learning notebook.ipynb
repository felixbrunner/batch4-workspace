{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxgRMERIM2IU"
   },
   "source": [
    "# SLU12- Support Vector Machines: Learning notebook\n",
    "\n",
    "In this notebook we will be covering the following:\n",
    "\n",
    "\n",
    "*  Hyperplanes\n",
    "*  Maximal Margin Classifier\n",
    "* Support Vector Classifier\n",
    "* Support Vector Machine\n",
    "* Multi-Class extension\n",
    "* Support Vector Regression\n",
    "\n",
    "New tools in this unit\n",
    "\n",
    "* [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "* [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ctwokYS3Mdn0",
    "outputId": "f1caf272-1bce-49d0-fd9f-08c7be7a89d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.13299459, -0.62847048,  0.58320678,  0.28130885],\n",
       "       [ 1.83321442, -0.62847048,  1.29902313,  0.93048312],\n",
       "       [ 0.78288467, -0.62847048,  0.47308119,  0.4111437 ],\n",
       "       [-1.31777482,  0.31719043, -1.17880269, -1.2767094 ],\n",
       "       [ 1.3664012 ,  0.31719043,  0.52814398,  0.28130885]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# load the iris dataset and train-test split it\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# SVMs are not scale invariant, so we should scale our data beforehand\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Take a look at the first five observations' features\n",
    "X_train[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g70_dcpI_01V",
    "outputId": "4e13d18a-3e0f-4f57-91a3-39244d56dc28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first five observations' class\n",
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnMp9uxAOwDZ"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this learning unit, we discuss the support vector machine (SVM), an approach that was developed in the computer science community in\n",
    "the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one\n",
    "of the best “out of the box” shallow classifiers.\n",
    "\n",
    "![intro](media/into.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFB9j5bt2Sc2"
   },
   "source": [
    "People often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as “support vector machines”. To avoid confusion, we will carefully distinguish between these three notions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlQLNTk9X4W3"
   },
   "source": [
    "## Hyperplanes\n",
    "\n",
    "In a p-dimensional space, a hyperplane is a flat  subspace of dimension p − 1. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace—in other words, a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In p > 3\n",
    "dimensions, it can be hard to visualize a hyperplane, but the notion of a (p − 1)-dimensional flat subspace still applies.\n",
    "\n",
    "![hyperplanes](media/hyperplanes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60wGgq8Z345d"
   },
   "source": [
    "Consider a $n×p$ data matrix **X** that consists of n training observations in p-dimensional space. A hyperplane in the p-dimensional setting is defined by the below equation, \n",
    "\n",
    "$\\beta_{0}+ \\beta_{1}x_{1}+ ... +\\beta_{p}x_{p} = 0$\n",
    "\n",
    "Using this hyperplane, we can separate two classes of observations. For instance, all observations that satisfy\n",
    "\n",
    "$\\beta_{0}+ \\beta_{1}x_{1}+ ... +\\beta_{p}x_{p} > 0$\n",
    "\n",
    "will be classified as one class, while all other observations that satisfy \n",
    "\n",
    "$\\beta_{0}+ \\beta_{1}x_{1}+ ... +\\beta_{p}x_{p} < 0$\n",
    "\n",
    "will be classified as the remaining class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7hCW7g6ZVum"
   },
   "source": [
    "## Maximal Margin Classifier (MMC)\n",
    "\n",
    "The Maximal Margin Classifier is a binary classification method that makes use of the **optimal separating hyperplane**, which is the separating hyperplane that is farthest from the training observations. The minimal distance from the observations to the hyperplane is known as the **margin**. The observations that lie on the margin are known as the **support vectors**. The figure below illustrates the decision boundary (black line), the margin (dashed lines), and the support vectors (points on the dashed line) of the application of a maximal margin classifier.\n",
    "\n",
    "![mmc](media/mmc.png)\n",
    "\n",
    "In order to construct the optimal separating hyperplane based on a set of n training observations $x_{1}, . . . , x_{n} ∈ R_{p}$ and associated class labels $y_{1}, . . . , y_{n} $∈ {-1,1}, we must find the solution to the optimization problem\n",
    "\n",
    "* maximize $M$\n",
    "\n",
    "* subject to $\\sum_{j=1}^{p} β^{2}=1$\n",
    "\n",
    "* $y_{i}(\\beta_{0}+ \\beta_{1}x_{i1}+ ... +\\beta_{p}x_{ip}) =  M,    ∀ i = 1, . . . , n$ \n",
    "\n",
    "Here, M represents the margin, and the optimization problem chooses $\\beta_{0}+ \\beta_{1} + ... + \\beta_{p}$ to maximize M.\n",
    "\n",
    "The maximal margin classifier is a very natural way to perform classification, if a separating hyperplane exists. However, in many cases no separating hyperplane exists, and so there is no maximal margin classifier. In this case, the optimization problem has no solution with M >0.\n",
    "\n",
    "![meme](media/unseparable_meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRbhkeVvaSs8"
   },
   "source": [
    "## Support Vector Classifier (SVC)\n",
    "\n",
    "The Support Vector Classifier, also known as the soft margin classifier, is an extension of the maximal margin classifier. The soft margin allows some observations to be on the incorrect side of the margin, or even on the incorrect side of the hyperplane. \n",
    "\n",
    "This overcomes the MMC's limitation of not being able to handle cases where no separating hyperplane exists. In fact, even if an  separating hyperplane does exist, there are instances in which a classifier based on such a separating hyperplane might not be desirable. An MMC classifier based on a separating hyperplane will necessarily perfectly classify all of the training observations, which can lead to sensitivity to individual observations.\n",
    "\n",
    "By not perfectly separating the two classes, the SVC has greater robustness to individual observations and a better classification of *most* of the observations. The image below illustrates the decision boundary and margin of an SVC, \n",
    "\n",
    "![svc](media/svc.png)\n",
    "\n",
    "As can be seen, some observations violate the hyperplane. The method to obtain this hyperplane is similar to that of the MMC, except that we now introduce some slack variables $s_{0}, ..., s_{n}$ and a non-negative tuning paramenter C.\n",
    "\n",
    "* maximize M \n",
    "\n",
    "* subject to $\\sum_{j=1}^{p} β^{2}=1$\n",
    "\n",
    "* $y_{i}(\\beta_{0}+ \\beta_{1}x_{i1}+ ... +\\beta_{p}x_{ip}) ≥ M(1-s_{i})$\n",
    "* $s_{i} ≥ 0$\n",
    "\n",
    "* $\\sum_{j=1}^{p} s_{i} ≤ C$\n",
    "\n",
    "* ∀ i = 1, . . . , n\n",
    "\n",
    "\n",
    "The slack variables $s_{0}, ..., s_{n}$  allow individual observations to be on the wrong side of the margin or the hyperplane. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7V5T0HUaRrJA",
    "outputId": "2a58db42-44a8-4d64-93ab-5157ce8eb4e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the SVC class\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an estimator\n",
    "linear_svc = SVC(kernel=\"linear\", C=1) # don't worry about the kernel argument for now\n",
    "linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YesYso4i4UUG",
    "outputId": "dab3c583-0244-45a6-fbc5-971c4b55e826"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 2, 0, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "       2, 1, 2, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the estimator\n",
    "linear_svc.fit(X_train, y_train)\n",
    "# make predictions\n",
    "predictions = linear_svc.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w4klJ7Y77Aef",
    "outputId": "b1e1f35b-7060-4834-f1ac-58fbb9e5582e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score the estimator\n",
    "linear_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "YJVYydj_-G7b",
    "outputId": "cc861407-edc0-4c58-9681-e836cc660c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50085169,  0.79002088, -1.23386549, -1.01703969],\n",
       "       [-0.96766491,  1.02643611, -1.17880269, -0.75736998],\n",
       "       [-1.55118143, -1.81054661, -1.34399108, -1.14687454],\n",
       "       [ 1.13299459, -0.62847048,  0.58320678,  0.28130885],\n",
       "       [ 0.78288467, -0.62847048,  0.47308119,  0.4111437 ],\n",
       "       [ 0.19936815, -0.8648857 ,  0.74839517,  0.54097856],\n",
       "       [-0.8509616 , -1.33771615, -0.40792355, -0.10819571],\n",
       "       [-1.08436821, -1.57413138, -0.24273516, -0.23803057],\n",
       "       [ 0.43277476, -2.04696183,  0.41801839,  0.4111437 ],\n",
       "       [-0.50085169, -0.15564002,  0.41801839,  0.4111437 ],\n",
       "       [ 1.01629128, -0.15564002,  0.69333237,  0.67081341],\n",
       "       [ 0.19936815, -0.39205525,  0.41801839,  0.4111437 ],\n",
       "       [ 0.54947806, -1.33771615,  0.63826958,  0.4111437 ],\n",
       "       [-0.26744507, -0.15564002,  0.41801839,  0.4111437 ],\n",
       "       [ 0.31607145, -0.39205525,  0.52814398,  0.28130885],\n",
       "       [ 0.54947806,  0.55360565,  0.52814398,  0.54097856],\n",
       "       [ 0.54947806, -0.8648857 ,  0.63826958,  0.80064827],\n",
       "       [ 0.66618137,  0.0807752 ,  0.96864635,  0.80064827],\n",
       "       [ 0.43277476, -0.62847048,  0.58320678,  0.80064827],\n",
       "       [ 0.19936815, -0.15564002,  0.58320678,  0.80064827],\n",
       "       [-0.26744507, -0.62847048,  0.63826958,  1.06031798],\n",
       "       [ 0.31607145, -1.10130093,  1.02370915,  0.28130885],\n",
       "       [ 0.54947806, -0.62847048,  0.74839517,  0.4111437 ],\n",
       "       [ 0.19936815, -2.04696183,  0.69333237,  0.4111437 ],\n",
       "       [ 1.59980781, -0.15564002,  1.13383474,  0.54097856],\n",
       "       [ 0.08266484, -0.15564002,  0.74839517,  0.80064827],\n",
       "       [ 0.31607145, -0.15564002,  0.63826958,  0.80064827]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the support vectors\n",
    "linear_svc.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fiAph15Rrlo"
   },
   "source": [
    "### The C Penalty\n",
    "\n",
    "The C penalty is the sum of all the slack variables, and it thus determines the number and severity of the violations to the margin (and the hyperplane) that the model will tolerate.  If C=0 then we are back to the MMC. As C increases, the model becomes more tolerant and thus the margin will increase. this is illustrated by the pictures below, \n",
    "\n",
    "![small_C](media/small_C.png)\n",
    "![large_C](media/large_C.png)\n",
    "\n",
    "The picture on the top illustrates an SVC with a small value of C, while the picture on the bottom illustrates an SVC with a large value of C. In practice, C is often chosen via cross-validation, though a value of C=1 is usually a good start.\n",
    "\n",
    "C controls the bias-variance trade-off of the statistical learning technique. When C is small  the  classifier is highly fit to the data, which may have low bias but high variance. On the other hand, when C is larger, the margin is wider and we allow more violations to it. This results in fitting the data less hard and obtaining a classifier that is potentially more biased but with lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMXCPKf4aoja"
   },
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "So far we have only considered models with a linear decision boundary. Support Vector Machines are the extension of the SVC to the non-linear case. The full details of this extension are somewhat complex and beyond the scope of this learning unit, but the main ideas are detailed below. \n",
    "\n",
    "### Kernels\n",
    "\n",
    "SVMs enlarge the feature space in a specific way using **kernels**. The kernel approach is simply an efficient computational approach for acommodating non-linear decision boundaries. A kernel function quantifies the similarity of two observations. For instance, to obtain the SVC we could use the following kernel,\n",
    "\n",
    "\n",
    "$K(x_{i}, x_{n}) =  \\sum_{j=1}^{p}x_{ij}x_{nj}$\n",
    "\n",
    "\n",
    "Kernels operate in implicit feature space without ever computing the coordinates of the data in that space. In practice, they simply compute the inner products between the images of all pairs of data in the feature space. This is known as the **Kernel Trick**. \n",
    "\n",
    "![kernel_trick](media/kernel_trick.png)\n",
    "\n",
    "Data not linearly separable in n-dimensional space may be linearly separable in higher dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LfIRqGu4Iu8"
   },
   "source": [
    "\n",
    "One popular choice is the **polynomial kernel** of degree d, given by the equation below,\n",
    "\n",
    "$K(x_{i}, x_{n}) = (1 + \\sum_{j=1}^{p}x_{ij}x_{nj})^{d}$\n",
    "\n",
    "![poly_kernel](media/poly_kernel.png)\n",
    "\n",
    "Using such a kernel with d > 1, instead of the SVC, leads to a much more flexible decision boundary. It essentially allows to fitting a support vector\n",
    "classifier in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "nl-pzgpo4L-L",
    "outputId": "9b4816db-ac2a-4895-81f3-491628ac00f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass kernel='poly' and degree=d to create an\n",
    "# SVM with polynomial kernel of degree d\n",
    "# For example, let's use a kernel with a degree of 3\n",
    "polynomial_svm = SVC(kernel=\"poly\", degree=3)\n",
    "polynomial_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TStsBmjPAS9X",
    "outputId": "97025c88-2b62-4996-c2e3-d07327b953be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "polynomial_svm.fit(X_train, y_train)\n",
    "# score the model\n",
    "polynomial_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFOc9jmx4MZT"
   },
   "source": [
    "Another popular option is the **radial kernel**, \n",
    "\n",
    "$K(x_{i}, x_{n}) = \\exp(-\\gamma\\sum_{j=1}^{p}(x_{ij}-x_{nj})^{2})$\n",
    "\n",
    "Where  $\\gamma$  is a positive constant. This kernel has very local behavior, meaning that the classification of new observations will be mostly determined by observations very close to it. The below picture shows the application of a radial kernel, \n",
    "\n",
    "![rbf_kernel](media/rbf_kernel.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Ndpj8ipZ5k2h",
    "outputId": "a607cec3-7af9-4960-f0af-0f62b7aea044"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass kernel='rbf' (default) to create an \n",
    "# SVM with radial kernel\n",
    "radial_svm = SVC(kernel=\"rbf\")\n",
    "radial_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eh3FtQyuAi-r",
    "outputId": "0a31deae-14ce-445c-e750-8b6018d5b6f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "radial_svm.fit(X_train, y_train)\n",
    "# score the model\n",
    "radial_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsP5DSPpa2ir"
   },
   "source": [
    "### Multi-Class Classification\n",
    "\n",
    "So far the estimators we have seen are only applicable to binary classification. There are ways to extend them in order to perform multi-class classification, and two popular methods are One-Vs-one (OVO) and One-Vs-Rest (OVR)\n",
    "\n",
    "\n",
    "One-Vs-One\n",
    "1. Fit $\\binom{K}{2}$ SVMs, each comparing a pair of classes \n",
    "2. Assign test observation to most frequently assigned class in all pairwise classifications\n",
    "\n",
    "One-Vs-Rest\n",
    "1. Fit K SVMs\n",
    "2. Compare each K class to remaining K-1 classes \n",
    "2. Assign  test observation for which $\\beta_{0}+ \\beta_{1}x_{1}+ ... +\\beta_{p}x_{p}$ is largest (this amounts to a high level of confidence that the test\n",
    "observation belongs to the kth class rather than to any of the other classes) \n",
    "\n",
    "Luckily,  all sklearn SVM estimators already implement multi-class classification, so we don't need to do it ourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8SZt2wqqPo7S",
    "outputId": "3d003117-9a6b-4b1d-f9ef-a6306fbbcc57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can specify which multi-class method you want your estimator to use\n",
    "# through the \"decision_function_shape\" argument.\n",
    "SVC(decision_function_shape=\"ovo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0CLBoUqql8O"
   },
   "source": [
    "## Support Vector Regression\n",
    "\n",
    "\n",
    "The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\n",
    "\n",
    "The model produced by support vector classification  depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data. This is illustrated by the image below,\n",
    "\n",
    "![SVR](media/svr.png)\n",
    "\n",
    "The model fits a decision boundary line for which the support vectors are within a certain deviation. Then, that line is used to compute the predictions. For this, we can use the [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)  implementation from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gd5wYqrmqi9p",
    "outputId": "3b4628a7-24c7-403a-d6e3-38a09e6e0faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Boston Dataset (Regression)\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Import the SVR estimator\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create the SVR estimator\n",
    "svr = SVR()\n",
    "svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "wsPapQsnDYOH",
    "outputId": "821bd78d-838b-435c-b97b-a451a98a0fa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.08737642, 20.97213554, 22.22923163, 20.00121374, 24.50894129,\n",
       "       24.56039279, 23.09991076, 19.28923481, 16.48237324, 15.80467939,\n",
       "       16.11848432, 23.40314605, 23.10961278, 22.42506355, 23.94890945,\n",
       "       16.52495859, 23.35075638, 23.37493559, 16.68368905, 23.28147835,\n",
       "       24.0650758 , 16.15501834, 23.25196236, 16.27298916, 22.93099086,\n",
       "       13.43827734, 23.13422746, 13.61795695, 16.01496326, 14.12314288,\n",
       "       13.20441192, 24.73597565, 24.01814284, 23.69483848, 16.2864095 ,\n",
       "       23.82504499, 21.62541574, 22.26996653, 24.02623962, 23.31698904,\n",
       "       18.39196048, 23.22519716, 16.72657384, 20.7691608 , 13.22392857,\n",
       "       15.98099752, 23.3973582 , 23.29477609, 23.25421948, 13.33532693,\n",
       "       20.90563222, 22.8547802 , 15.42040327, 19.42055522, 22.0834804 ,\n",
       "       23.34153846, 22.07588512, 16.03614602, 16.04367732, 19.39801282,\n",
       "       22.39104318, 15.78306307, 16.38651339, 21.77079877, 22.39455072,\n",
       "       23.65753589, 21.11532191, 20.43630859, 23.53445861, 19.4034815 ,\n",
       "       20.58612607, 24.1460597 , 22.76166393, 13.38801098, 21.32749423,\n",
       "       16.12122983, 21.82220204, 23.24316502, 15.33430532, 18.94609072,\n",
       "       22.46443261, 23.50127649, 24.83525992, 16.18969064, 24.7859724 ,\n",
       "       15.79826952, 19.86302193, 23.06405963, 16.3188061 , 22.53940641,\n",
       "       13.28100236, 16.14646925, 22.27026248, 23.00851222, 24.32845057,\n",
       "       22.65222257, 24.21252745, 24.00903484, 15.3245459 , 16.18224214,\n",
       "       24.66464009, 15.38158557])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the SVR estimator\n",
    "svr.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9pZKnM3WCpIM",
    "outputId": "57024901-281a-415b-c21b-6d6a54470218"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2952830553310496"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score the estimator (using R^2)\n",
    "svr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YmdLkYKA-Q9"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it for Support Vector Machines! below is the recap of the main points of this SLU:\n",
    " \n",
    "\n",
    "*   p-dimensional hyperplanes can be used as decision boundaries\n",
    "*   Maximal Margin Classifier uses the optimal separating hyperplane\n",
    "*   SVC uses a soft margin to allow misclassifications\n",
    "*   SVM uses kernels to handle non-linearity\n",
    "*   Extension to multi-class can be achieved with OVO and OVR\n",
    "*   Extension to regression can be done with SVR\n",
    "\n",
    "### Further readings\n",
    "\n",
    "[The Kernel method](https://en.wikipedia.org/wiki/Kernel_method)\n",
    "\n",
    "[Support Vector Regression](https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff)\n",
    "\n",
    " Witten, Daniela, et al. “Chapter 9.” An Introduction to Statistical Learning, by Gareth James, Springer, 2017.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Learning notebook - SLU12 (Support Vector Machines).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
